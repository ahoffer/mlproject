---
title: "Practical Machine Learning Course Project"
output: html_document
---

This report describes how classification was performed on a human activity recognition data set. Data from sensor devices is classified in one of five ways (classes A, B C, D, E). The type of sensor data, its timestamps and the meaning of the classes is not essential to the project and is not discussed in this report. Instead, the report takes the read through the steps ofdata analysis: 

* Define goal
* Aquire data
* Define features
* Train model
* Validate model
* Predict classes of unlabeled data

## Define goal
Given a set of time series sensor data, decide if a person is performing an specific activity (weight lifting) correctly, or if the person performing the task incorrectly. If the task is being performed incorrectly, determine what the mistake is. In this data set, class A means the task was performed correctly and classes B-E are different kinds of errors.  

Train a machine learning model and use cross-fold validation to determine its accurary and kappa values. Also, predict the class for 20 unlabelled features.

##Acquire the data
Download the training and test sets from the Internet.
```{r}
# Data was downloaded previously therefore next two statements are commented out.
# download.file(destfile="training.csv", url='https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
# download.file(destfile = 'test.csv', url='https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')

#Load data into workspace
train = read.csv('training.csv', stringsAsFactors=FALSE)
test = read.csv('test.csv', stringsAsFactors=FALSE)
```

## Define features
I have chosen to not use the timestamp and window information in the data set. Modeling the data as time series could lead to better, more powerful models, but the extra effort is not justified for this assignment. Also, the unlabeled test data is has time stamps, but it not ordered in time. It looks like features were extracted from the data set at random. Training a time servies model would not benefit predictions made on the test data.
```{r}
#Remove observation ID, subject name, and timestamp/window information
#With only 20 items in the test set, it doesn't make much sense to treat them as time series data.
train = train[,-(1:8)]
#For test set, also remove problem ID
test = test[,-c(1:8, 160)]

#Data frame still has a lot of garbage in the form empty strings and NAs:
#$ skewness_yaw_belt       : chr  "" "" "" "" ...
#$ max_roll_belt           : num  NA NA NA NA NA NA NA NA NA NA ...

#Remove any atribute with more than 50% NA or empty strings
getGetFeatureAttributes = function(df) { 
  cutoff = 0.5 * nrow(df)
  good_cols1 = colSums(is.na(df)) < cutoff 
  #Define helper function. Return true if input is an empty string
  equalsEmptyString = function(x) x==""
  good_cols2 = colSums(equalsEmptyString(df)) < cutoff
  good_cols_all = good_cols1 & good_cols2
  return(good_cols_all)
}

test = test[,getGetFeatureAttributes(test)]
train = train[, getGetFeatureAttributes(train)]

#The only non-numeric attribute left is the classification or "classe"
#Caret gets pissy if the classes are characters and not factors.
train$classe = as.factor(train$classe)
```

```{r include=FALSE}
### Setup R environment
# If packages not installed, uncomment following three statements.
# install.packages("caret")
# install.packages('e1071')
# install.packages'doMC')
library(caret)
library(e1071)
library(doMC)
```

## Train Model
We will create two models, a random forest and a graident boosting machine. To both evualte the model for out of sample error, we will ask R to perform cross-validation with 7 folds. The test data is divided into seven groups. Each time the model is trained, one group is held-out as a validation set. The effect is that the model evaluated against seven different groups and that is how its out-of-sample error is determined. 
```{r}
#Run the models using parallel processing.
registerDoMC(cores = 6)

#This might make the results reproducible
set.seed(1)

#Use cross fold validation with 7 folds.
trainCtrl = trainControl(method='cv', number=7)

#Define helper function to test different machine learning algorithms
learn = function(x) train(classe ~ ., data=train, method=x, trControl=trainCtrl, preProcess=c("scale", "center"))

#Create some models
gbm_model = learn('gbm')
rf_model = learn('rf')
```

## Validate models
The models are validated by two metrics: accuracy and "kappa". Accuracy is somple the persentage of observations that were correctly classified. However, even if we randomly guessed, we would expect an accuracy greater than zero because sometimes we would guess correctly. The kappa takes into account the frequency different classes appears in the data and determines how much better the accuracy of the model is over random guessing. A kappa score of zero means the model is not better than guessing. A kappa score of 100% means the model had perfect acurracy. 
```{r}
#Look at accuracy and kappa
gbm_model$results
rf_model$results
```

The accuracy and kappa are very, very good for both models. Random forest was the winner. When the number of trees was set to 100 and the number of variables per level (mtry) set to 26, the kappa was 0.994. The standard deviation of the kappa was just 0.002. In most circumstances, that would indicate the model is overfit, but the cross validation with 7 folds suggests the out-sample-error will be low, even as low as 1-accuracy, which is just 0.6%.

Accuracy is only one measure of quality. The confusion matrix is a way to understand how well the model performed for each class it predicted. The the number of correct number of instances for each class is the sum of a column. The the number times the model predicted a particular class is the sum of a row. In the confusion matrix below
```{r}
confusionMatrix(rf_model)
```

## Predict classes of unlabeled data
This section is not included in the report.
